{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Mining & NLP with NLTK\n",
    "-----\n",
    "In today, I am going to mention some NLP techniques These are;\n",
    "\n",
    "    1.1. Tokenization\n",
    "    1.2. Stemming\n",
    "    1.3. Lemmatization\n",
    "    1.4. Stopwords Removel\n",
    "    1.5. Bag of Words models\n",
    "    1.6. Parts of Speech (POS)\n",
    "    1.7. Named Entity Recognization (NER)\n",
    "    1.8. Chunking\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tokenization\n",
    "-----------\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = \"When most people hear the term artificial intelligence, the first thing they usually think of is robots. That's because big-budget films and novels weave stories about human-like machines that wreak havoc on Earth. But nothing could be further from the truth. The goals of artificial intelligence include mimicking human cognitive activity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word and Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When most people hear the term artificial intelligence, the first thing they usually think of is robots.\n",
      "That's because big-budget films and novels weave stories about human-like machines that wreak havoc on Earth.\n",
      "But nothing could be further from the truth.\n",
      "The goals of artificial intelligence include mimicking human cognitive activity.\n"
     ]
    }
   ],
   "source": [
    "AI_sent_tokenizes = sent_tokenize(AI)\n",
    "\n",
    "for AI_sent_tokenize in AI_sent_tokenizes:\n",
    "    print(AI_sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'most', 'people', 'hear', 'the', 'term', 'artificial', 'intelligence', ',', 'the', 'first', 'thing', 'they', 'usually', 'think', 'of', 'is', 'robots', '.', 'That', \"'s\", 'because', 'big-budget', 'films', 'and', 'novels', 'weave', 'stories', 'about', 'human-like', 'machines', 'that', 'wreak', 'havoc', 'on', 'Earth', '.', 'But', 'nothing', 'could', 'be', 'further', 'from', 'the', 'truth', '.', 'The', 'goals', 'of', 'artificial', 'intelligence', 'include', 'mimicking', 'human', 'cognitive', 'activity', '.']\n"
     ]
    }
   ],
   "source": [
    "AI_word_tokenizes = word_tokenize(AI)\n",
    "print(AI_word_tokenizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 4, '.': 4, 'artificial': 2, 'intelligence': 2, 'of': 2, 'that': 2, 'when': 1, 'most': 1, 'people': 1, 'hear': 1, ...})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = FreqDist()\n",
    "\n",
    "for AI_word_tokenize in AI_word_tokenizes:\n",
    "    fdist[AI_word_tokenize.lower()] += 1\n",
    "\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('.', 4), ('artificial', 2), ('intelligence', 2), ('of', 2)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top5 = fdist.most_common(5)\n",
    "\n",
    "fdist_top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"When most people hear the term artificial intelligence, the first thing they usually think of is robots.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_word_tokenize = word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_word_tokenize_bigrams = list(nltk.bigrams(sequence=string_word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When', 'most'),\n",
       " ('most', 'people'),\n",
       " ('people', 'hear'),\n",
       " ('hear', 'the'),\n",
       " ('the', 'term'),\n",
       " ('term', 'artificial'),\n",
       " ('artificial', 'intelligence'),\n",
       " ('intelligence', ','),\n",
       " (',', 'the'),\n",
       " ('the', 'first'),\n",
       " ('first', 'thing'),\n",
       " ('thing', 'they'),\n",
       " ('they', 'usually'),\n",
       " ('usually', 'think'),\n",
       " ('think', 'of'),\n",
       " ('of', 'is'),\n",
       " ('is', 'robots'),\n",
       " ('robots', '.')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_word_tokenize_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_word_tokenize_trigrams = list(nltk.trigrams(sequence=string_word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When', 'most', 'people'),\n",
       " ('most', 'people', 'hear'),\n",
       " ('people', 'hear', 'the'),\n",
       " ('hear', 'the', 'term'),\n",
       " ('the', 'term', 'artificial'),\n",
       " ('term', 'artificial', 'intelligence'),\n",
       " ('artificial', 'intelligence', ','),\n",
       " ('intelligence', ',', 'the'),\n",
       " (',', 'the', 'first'),\n",
       " ('the', 'first', 'thing'),\n",
       " ('first', 'thing', 'they'),\n",
       " ('thing', 'they', 'usually'),\n",
       " ('they', 'usually', 'think'),\n",
       " ('usually', 'think', 'of'),\n",
       " ('think', 'of', 'is'),\n",
       " ('of', 'is', 'robots'),\n",
       " ('is', 'robots', '.')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_word_tokenize_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_word_tokenize_ngrams = list(nltk.ngrams(string_word_tokenize, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When', 'most', 'people', 'hear', 'the'),\n",
       " ('most', 'people', 'hear', 'the', 'term'),\n",
       " ('people', 'hear', 'the', 'term', 'artificial'),\n",
       " ('hear', 'the', 'term', 'artificial', 'intelligence'),\n",
       " ('the', 'term', 'artificial', 'intelligence', ','),\n",
       " ('term', 'artificial', 'intelligence', ',', 'the'),\n",
       " ('artificial', 'intelligence', ',', 'the', 'first'),\n",
       " ('intelligence', ',', 'the', 'first', 'thing'),\n",
       " (',', 'the', 'first', 'thing', 'they'),\n",
       " ('the', 'first', 'thing', 'they', 'usually'),\n",
       " ('first', 'thing', 'they', 'usually', 'think'),\n",
       " ('thing', 'they', 'usually', 'think', 'of'),\n",
       " ('they', 'usually', 'think', 'of', 'is'),\n",
       " ('usually', 'think', 'of', 'is', 'robots'),\n",
       " ('think', 'of', 'is', 'robots', '.')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_word_tokenize_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Stemming\n",
    "--------------\n",
    "Stemming is a process of reducing a word to its word setem that affix to suffix and prefix or to do roots of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include mimicking human cognitive activity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming: \n",
      "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include mimicking human cognitive activity.\n",
      "----------------\n",
      "After stemming: \n",
      "['artifici intellig is base on the principl that human intellig can be defin in a way that a machin can easili mimic it and execut task from the most simpl to those that are even more complex the goal of artifici intellig includ mimick human cognit activ']\n"
     ]
    }
   ],
   "source": [
    "compile_ = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "string_ = re.sub(\"[^a-zA-Z]\", \" \", str(string))\n",
    "string_ = string_.lower().split()\n",
    "string_ = [ps.stem(word=word_) for word_ in string_]\n",
    "string_ = \" \".join(string_)\n",
    "compile_.append(string_)\n",
    "\n",
    "print(\"Before stemming: \\n{}\".format(string))\n",
    "print(\"----------------\")\n",
    "print(\"After stemming: \\n{}\".format(compile_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming: \n",
      "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include mimicking human cognitive activity.\n",
      "----------------\n",
      "After stemming: \n",
      "['art intellig is bas on the principl that hum intellig can be defin in a way that a machin can easy mim it and execut task from the most simpl to thos that ar ev mor complex the goal of art intellig includ mimick hum cognit act']\n"
     ]
    }
   ],
   "source": [
    "compile_ = []\n",
    "\n",
    "lts = LancasterStemmer()\n",
    "\n",
    "string_ = re.sub(\"[^a-zA-Z]\", \" \", str(string))\n",
    "string_ = string_.lower().split()\n",
    "string_ = [lts.stem(word=word_) for word_ in string_]\n",
    "string_ = \" \".join(string_)\n",
    "compile_.append(string_)\n",
    "\n",
    "print(\"Before stemming: \\n{}\".format(string))\n",
    "print(\"----------------\")\n",
    "print(\"After stemming: \\n{}\".format(compile_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Lematization\n",
    "--------------\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include mimicking human cognitive activity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatizer: \n",
      "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include mimicking human cognitive activity.\n",
      "----------------\n",
      "After lemmatizer: \n",
      "['artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute task from the most simple to those that are even more complex the goal of artificial intelligence include mimicking human cognitive activity']\n"
     ]
    }
   ],
   "source": [
    "compile_ = []\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "string_ = re.sub(\"[^a-zA-Z]\", \" \", str(string))\n",
    "string_ = string_.lower().split()\n",
    "string_ = [lemma.lemmatize(word=word_) for word_ in string_]\n",
    "string_ = \" \".join(string_)\n",
    "compile_.append(string_)\n",
    "\n",
    "print(\"Before lemmatizer: \\n{}\".format(string))\n",
    "print(\"----------------\")\n",
    "print(\"After lemmatizer: \\n{}\".format(compile_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Stopwords\n",
    "----------\n",
    "Stopwords are the words in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Bag of Words Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = \"When most people hear the term artificial intelligence, the first thing they usually think of is robots. That's because big-budget films and novels weave stories about human-like machines that wreak havoc on Earth. But nothing could be further from the truth. The goals of artificial intelligence include mimicking human cognitive activity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When most people hear the term artificial intelligence, the first thing they usually think of is robots. That's because big-budget films and novels weave stories about human-like machines that wreak havoc on Earth. But nothing could be further from the truth. The goals of artificial intelligence include mimicking human cognitive activity.\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_sent_tokenizes = sent_tokenize(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When most people hear the term artificial intelligence, the first thing they usually think of is robots.',\n",
       " \"That's because big-budget films and novels weave stories about human-like machines that wreak havoc on Earth.\",\n",
       " 'But nothing could be further from the truth.',\n",
       " 'The goals of artificial intelligence include mimicking human cognitive activity.']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_sent_tokenizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_ = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for AI_sent_tokenize in AI_sent_tokenizes:\n",
    "\n",
    "    string_ = re.sub(\"[^a-zA-Z]\", \" \", str(AI_sent_tokenize))\n",
    "    string_ = string_.lower().split()\n",
    "    string_ = [ps.stem(word=word_) for word_ in string_ if not word_ is set(stopwords.words(\"english\"))]\n",
    "    string_ = \" \".join(string_)\n",
    "    compile_.append(string_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when most peopl hear the term artifici intellig the first thing they usual think of is robot',\n",
       " 'that s becaus big budget film and novel weav stori about human like machin that wreak havoc on earth',\n",
       " 'but noth could be further from the truth',\n",
       " 'the goal of artifici intellig includ mimick human cognit activ']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(compile_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vect.fit_transform(compile_).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 2, 1, 1, 1, 0, 1, 0, 1,\n",
       "        0],\n",
       "       [1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=results, columns=[columns_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>activ</th>\n",
       "      <th>and</th>\n",
       "      <th>artifici</th>\n",
       "      <th>be</th>\n",
       "      <th>becaus</th>\n",
       "      <th>big</th>\n",
       "      <th>budget</th>\n",
       "      <th>but</th>\n",
       "      <th>cognit</th>\n",
       "      <th>...</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>they</th>\n",
       "      <th>thing</th>\n",
       "      <th>think</th>\n",
       "      <th>truth</th>\n",
       "      <th>usual</th>\n",
       "      <th>weav</th>\n",
       "      <th>when</th>\n",
       "      <th>wreak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  about activ and artifici be becaus big budget but cognit  ... that the they  \\\n",
       "0     0     0   0        1  0      0   0      0   0      0  ...    0   2    1   \n",
       "1     1     0   1        0  0      1   1      1   0      0  ...    2   0    0   \n",
       "2     0     0   0        0  1      0   0      0   1      0  ...    0   1    0   \n",
       "3     0     1   0        1  0      0   0      0   0      1  ...    0   1    0   \n",
       "\n",
       "  thing think truth usual weav when wreak  \n",
       "0     1     1     0     1    0    1     0  \n",
       "1     0     0     0     0    1    0     1  \n",
       "2     0     0     1     0    0    0     0  \n",
       "3     0     0     0     0    0    0     0  \n",
       "\n",
       "[4 rows x 45 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Parts of Speech (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://m-clark.github.io/text-analysis-with-R/topic-modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_word_tokenizes = word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Artificial', 'JJ')]\n",
      "[('intelligence', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('based', 'VBN')]\n",
      "[('on', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('principle', 'NN')]\n",
      "[('that', 'IN')]\n",
      "[('human', 'NN')]\n",
      "[('intelligence', 'NN')]\n",
      "[('can', 'MD')]\n",
      "[('be', 'VB')]\n",
      "[('defined', 'VBN')]\n",
      "[('in', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('way', 'NN')]\n",
      "[('that', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('machine', 'NN')]\n",
      "[('can', 'MD')]\n",
      "[('easily', 'RB')]\n",
      "[('mimic', 'NN')]\n",
      "[('it', 'PRP')]\n",
      "[('and', 'CC')]\n",
      "[('execute', 'NN')]\n",
      "[('tasks', 'NNS')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for string_word_tokenize in string_word_tokenizes:\n",
    "\n",
    "    print(nltk.pos_tag([string_word_tokenize]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Named Entity Recognition\n",
    "----------\n",
    "Named entity recognition (NER) ‒ also called entity identification or entity extraction ‒ is a natural language processing (NLP) technique that automatically identifies named entities in a text and classifies them into predefined categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Born and raised in Madeira, Ronaldo began his senior club career playing for Sporting CP, before signing with Manchester United in 2003, aged 18. After winning the FA Cup in his first season, he helped United win three successive Premier League titles, the UEFA Champions League, and the FIFA Club World Cup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Born', 'and', 'raised', 'in', 'Madeira', ',', 'Ronaldo', 'began', 'his', 'senior', 'club', 'career', 'playing', 'for', 'Sporting', 'CP', ',', 'before', 'signing', 'with', 'Manchester', 'United', 'in', '2003', ',', 'aged', '18', '.', 'After', 'winning', 'the', 'FA', 'Cup', 'in', 'his', 'first', 'season', ',', 'he', 'helped', 'United', 'win', 'three', 'successive', 'Premier', 'League', 'titles', ',', 'the', 'UEFA', 'Champions', 'League', ',', 'and', 'the', 'FIFA', 'Club', 'World', 'Cup']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Born', 'NNP'), ('and', 'CC'), ('raised', 'VBN'), ('in', 'IN'), ('Madeira', 'NNP'), (',', ','), ('Ronaldo', 'NNP'), ('began', 'VBD'), ('his', 'PRP$'), ('senior', 'JJ'), ('club', 'NN'), ('career', 'NN'), ('playing', 'NN'), ('for', 'IN'), ('Sporting', 'VBG'), ('CP', 'NNP'), (',', ','), ('before', 'IN'), ('signing', 'VBG'), ('with', 'IN'), ('Manchester', 'NNP'), ('United', 'NNP'), ('in', 'IN'), ('2003', 'CD'), (',', ','), ('aged', 'VBD'), ('18', 'CD'), ('.', '.'), ('After', 'IN'), ('winning', 'VBG'), ('the', 'DT'), ('FA', 'NNP'), ('Cup', 'NNP'), ('in', 'IN'), ('his', 'PRP$'), ('first', 'JJ'), ('season', 'NN'), (',', ','), ('he', 'PRP'), ('helped', 'VBD'), ('United', 'NNP'), ('win', 'VB'), ('three', 'CD'), ('successive', 'JJ'), ('Premier', 'NNP'), ('League', 'NNP'), ('titles', 'NNS'), (',', ','), ('the', 'DT'), ('UEFA', 'NNP'), ('Champions', 'NNP'), ('League', 'NNP'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('FIFA', 'NNP'), ('Club', 'NNP'), ('World', 'NNP'), ('Cup', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPE Born/NNP)\n",
      "('and', 'CC')\n",
      "('raised', 'VBN')\n",
      "('in', 'IN')\n",
      "(GPE Madeira/NNP)\n",
      "(',', ',')\n",
      "(PERSON Ronaldo/NNP)\n",
      "('began', 'VBD')\n",
      "('his', 'PRP$')\n",
      "('senior', 'JJ')\n",
      "('club', 'NN')\n",
      "('career', 'NN')\n",
      "('playing', 'NN')\n",
      "('for', 'IN')\n",
      "('Sporting', 'VBG')\n",
      "(ORGANIZATION CP/NNP)\n",
      "(',', ',')\n",
      "('before', 'IN')\n",
      "('signing', 'VBG')\n",
      "('with', 'IN')\n",
      "(PERSON Manchester/NNP United/NNP)\n",
      "('in', 'IN')\n",
      "('2003', 'CD')\n",
      "(',', ',')\n",
      "('aged', 'VBD')\n",
      "('18', 'CD')\n",
      "('.', '.')\n",
      "('After', 'IN')\n",
      "('winning', 'VBG')\n",
      "('the', 'DT')\n",
      "('FA', 'NNP')\n",
      "('Cup', 'NNP')\n",
      "('in', 'IN')\n",
      "('his', 'PRP$')\n",
      "('first', 'JJ')\n",
      "('season', 'NN')\n",
      "(',', ',')\n",
      "('he', 'PRP')\n",
      "('helped', 'VBD')\n",
      "(GPE United/NNP)\n",
      "('win', 'VB')\n",
      "('three', 'CD')\n",
      "('successive', 'JJ')\n",
      "('Premier', 'NNP')\n",
      "('League', 'NNP')\n",
      "('titles', 'NNS')\n",
      "(',', ',')\n",
      "('the', 'DT')\n",
      "(ORGANIZATION UEFA/NNP)\n",
      "('Champions', 'NNP')\n",
      "('League', 'NNP')\n",
      "(',', ',')\n",
      "('and', 'CC')\n",
      "('the', 'DT')\n",
      "(ORGANIZATION FIFA/NNP Club/NNP)\n",
      "('World', 'NNP')\n",
      "('Cup', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "name_entities = ne_chunk(pos_tags)\n",
    "\n",
    "for name_entity in name_entities:\n",
    "    print(name_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Chunking\n",
    "-----\n",
    "Chunking in NLP is a process to take small pieces of information and group them into large units. The primary use of Chunking is making groups of \"noun phrases.\" It is used to add structure to the sentence by following POS tagging combined with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Born and raised in Madeira, Ronaldo began his senior club career playing for Sporting CP, before signing with Manchester United in 2003, aged 18. After winning the FA Cup in his first season, he helped United win three successive Premier League titles, the UEFA Champions League, and the FIFA Club World Cup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos_tag = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking = \"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = RegexpParser(chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cp.parse(tokens_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Born/NNP\n",
      "  and/CC\n",
      "  raised/VBN\n",
      "  in/IN\n",
      "  Madeira/NNP\n",
      "  ,/,\n",
      "  Ronaldo/NNP\n",
      "  began/VBD\n",
      "  his/PRP$\n",
      "  (NP senior/JJ club/NN)\n",
      "  (NP career/NN)\n",
      "  (NP playing/NN)\n",
      "  for/IN\n",
      "  Sporting/VBG\n",
      "  CP/NNP\n",
      "  ,/,\n",
      "  before/IN\n",
      "  signing/VBG\n",
      "  with/IN\n",
      "  Manchester/NNP\n",
      "  United/NNP\n",
      "  in/IN\n",
      "  2003/CD\n",
      "  ,/,\n",
      "  aged/VBD\n",
      "  18/CD\n",
      "  ./.\n",
      "  After/IN\n",
      "  winning/VBG\n",
      "  the/DT\n",
      "  FA/NNP\n",
      "  Cup/NNP\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  (NP first/JJ season/NN)\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  helped/VBD\n",
      "  United/NNP\n",
      "  win/VB\n",
      "  three/CD\n",
      "  successive/JJ\n",
      "  Premier/NNP\n",
      "  League/NNP\n",
      "  titles/NNS\n",
      "  ,/,\n",
      "  the/DT\n",
      "  UEFA/NNP\n",
      "  Champions/NNP\n",
      "  League/NNP\n",
      "  ,/,\n",
      "  and/CC\n",
      "  the/DT\n",
      "  FIFA/NNP\n",
      "  Club/NNP\n",
      "  World/NNP\n",
      "  Cup/NNP)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6326ae2dc7cd7629e7601181b1c625552adf2d5e9e5e96f66d5d4efbd1b9385"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
